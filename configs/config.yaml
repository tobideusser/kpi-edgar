# Base directory to store all task outputs
base_dir: "/scratch/data/edgar/icmla/v2"

################
# DATA PARSING #
################

DataParsing:
  path_to_data_folders: "/cluster/edgar_filings_above$200B"
  entity_prefixes: ["us-gaap"]
  entity_formats: ["ixt:numdotdecimal", "ix:nonFraction"]
  dataset_name: "EDGAR"
  debug_size: null


#####################
# DATA TOKENIZATION #
#####################

DataTokenizing:
  remove_tables: true
  language: &language "en"


################
# DATA TAGGING #
################

DataTagging:
  remove_non_currency_sentences: true
  language: *language


#####################
# Annotation Merger #
#####################

AnnotationMerging:
  excel_annotation_file: "/scratch/projects/edgar/data/kpi_edgar.xlsx"
  filter_for_annotated_docs: true  # set false for self-training
  ignore_noncritical_warnings: true
  skip_sentences_with_error: true
  parse_secondary_annotations: false
  save_as_json: false
  label_mapping:
    kpi: 'kpi'
    kpi_coref: 'kpi_coref'
    cy: 'cy'
    py: 'py'
    py1: "py1"
    increase: "increase" #'cy'
    increase_py: "increase_py" # "py"
    decrease: "decrease" # 'cy'
    decrease_py: "decrease_py" # "py"
    thereof: 'thereof'
    thereof_coref: 'thereof'
    attr: "attr"
    thereof_cy: 'cy'
    thereof_py: 'py'
    davon_increase: false
    davon_decrease: false
    false_positive: false

#####################
# SubWord Tokenization #
#####################

SubWordTokenization:
  # english: "bert-base-cased"
  # multilingual: "bert-base-multilingual-cased"
  # "ahmedrachid/FinancialBERT"
  tokenizer_name: &tokname "bert-base-cased" # **checking with this tokenizer 'bert-base-german-dbmdz-cased'
  special_tokens: null
#    additional_special_tokens : [['[NUM]']] ** it was giving error with this argument



##################
# MODEL TRAINING #
##################

ModelTraining:

  model_params:
    # choices: "untrained", "pretrained_decoder", "pretrained_full"
    type_: "untrained" #"pretrained_decoder"
    path_pretrained_model: "/scratch/data/edgar/banz_pretraining/ModelTraining/000" #/models/best_model.pt"

    encoder_params:
      # edgar w2v embeddings
#      - encoder_type_: "edgarW2V"
#        path_embedding: "/shared_with_lars_and_thiago/edgar/edgar-w2v-200d/edgar.word.w2v.200.bin"
#        embedding_dim: 200
#        oov_vector: 'random' # how should the out of vocabulary(oov) word should be handled
#        seed: 100
#        word_pooling: 'max' # this argument is not used just used here because it is used somewhere in model_training task file

#      # glove embeddings
#      - encoder_type_: "glove"
#        path_embedding: "/shared_with_lars_and_thiago/edgar/glove-6B/glove-6B-200d.txt"
#        embedding_dim: 200
#        oov_vector: 'random' # how should the out of vocabulary(oov) word should be handled
#        seed: 100
#        word_pooling: 'max' # this argument is not used just used here because it is used somewhere in model_training task file
#
      # tfidf embeddings
      - encoder_type_: "tfidf"
        path_embedding: "/shared_with_lars_and_thiago/edgar/tfIdf/tfIdf_2000.pkl" # this is where the embeddings by tfidf were saved. so anypath can be given here.
        embedding_dim: 2000
        word_pooling: 'max' # this argument is not used just used here because it is used somewhere in model_training task file


    #      # main bert sentence encoder
#      - encoder_type_: "sentenceEncoder"
#        type_: *tokname
#        finetune: true
#        word_pooling: 'rnn_local' # ['avg', 'rnn_local', 'attention', 'max'] # 'avg' # ['avg', 'rnn_local', 'attention' 'avg', 'max']
#        output_attentions: false
#        output_hidden_states: false

    decoder_params:
      ner_params:
#        - type_: 'span'
#          neg_sampling: 100
#          max_span_len: 10
#          span_len_embedding_dim: 25
#          pooling_fn: 'rnn_local' # 'max' # ['avg', 'rnn_local', 'attention', 'max']
#          dropout: 0.1
#          chunk_size: 1000
#          use_cls: false
#          loss_weight: 1  # [1., 1.3, '@loss_weight']
#          remove_overlapping_spans: false # true # [true, false]

        - type_: 'iobes'
          loss_weight: 1.  # [1., 1.3, '@loss_weight']
          use_cls: false  # [true, false]
          max_span_len: 100
          span_len_embedding_dim: 25
          pooling_fn:  'rnn_local' # ['rnn_local', 'attention', 'avg', 'max']
          use_ner_hidden_states: null
          decoding_params:
            - type_: 'rnn'
              dropout: 0.1
              label_embedding_dim: 128  # [64, 128, 256]
              label_masking: False
              model: 'gru'
              add_bos: False  # [True, False]

      re_params:
        neg_sampling: 100
        use_inbetween_context: true
        biaffine: false
        dropout: 0.1
        pooling_fn: 'rnn_local' # ['avg', 'rnn_local', 'attention', 'max'] # "rnn_local" # "avg" # ['avg', 'rnn_local', 'attention' 'avg', 'max']
        chunk_size: 1000
        threshold: 0.5
        loss_weight: 1  # [1., 0.7, '@loss_weight']
        filter_impossible_relations: false # [true, true, false, false] # true  # [true, true, true, false, false]  # only quantify improvement for best model
        remove_overlapping_relations: true # [true, false, true, false] # true  # [true, true, false, true, false]  # [true, false]

#    table_to_text_mode: true ** it was giving error with this argument

  dataloader_params:
    batch_size: 4  # [1, 2]
    drop_last: false

  optimizer_params:
    type_: 'adamW'
    lr: 1.e-5
    weight_decay: 0.01  # [0., 0.01]
    correct_bias: false

  lr_scheduler_params:
    type_: 'lin_warmup'
    interval: 'step'
    lr_warmup: 0.1

  evaluator_params:
#    ner_f1: {}
    ner_f1_adjusted:
      mode: "partial_type"
    re_f1_adjusted:
      mode: 'partial_type'

  trainer_params:
    num_epochs: 35
    num_grad_accumulation_steps: 1        # accumulate gradients over x batches before updating weights: simulates high batch size training without memory issues
    grad_norm: 1.  # [null, 1.0, null, '@clip']  # null                # scales gradient values such that the norm of combined grad vector equals grad_norm
    grad_clipping: null  # [null, null, 1.0, '@clip']                   # clips gradient values to range [-grad_clipping, +grad_clipping]
    early_stopping_patience: null  # 5
    valid_metric: '+re_micro_f1'

  seed: 42

####################
# MODEL PREDICTING #
####################

ModelPredicting:
  split_types: 'test'